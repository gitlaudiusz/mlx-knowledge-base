# Mlx Multimodal

Przewodnik konwersji modeli safetensors do formatu MLX-LM/VLM

Wprowadzenie

Wraz ze wzrostem popularności dużych modeli językowych i multimodalnych, pojawiła się potrzeba efektywnego uruchamiania ich na różnych platformach sprzętowych. Safetensors to format stworzony przez Hugging Face do bezpiecznego przechowywania wag modeli – w odróżnieniu od tradycyjnych plików .bin opartych o pickle, pliki safetensors zawierają wyłącznie dane tensora (parametry modelu) bez wykonywalnego kodu, co zwiększa bezpieczeństwo ￼. Safetensors są również szybkie w odczycie dzięki mechanizmowi zero-copy ￼. Z drugiej strony, MLX (Machine Learning eXchange) to nowy, otwarto-źródłowy stack AI od Apple, zoptymalizowany pod kątem układów Apple Silicon ￼. MLX udostępnia narzędzia do uruchamiania i trenowania modeli AI na Macach – w tym moduły MLX-LM (do modeli językowych) oraz MLX-VLM (do modeli wizualno-językowych). Konwersja modeli z formatu safetensors do formatu obsługiwanego przez MLX pozwala wykorzystać akcelerację sprzętową Apple (GPU/MPS, Neural Engine) i uruchamiać duże modele lokalnie z wysoką wydajnością.

W niniejszym przewodniku omówiono kompleksowo proces konwersji modeli w formacie safetensors do formatu MLX-LM/VLM. Przedstawimy techniki konwersji dla różnych typów modeli (NLP, wizja komputerowa, mowa), przeanalizujemy dostępne narzędzia i porównamy format MLX z innymi (PyTorch, TensorFlow, JAX). Sprawdzimy również poprawność implementacji konwersji na przykładzie modelu Gemma-3-27B-PT w skrypcie convert_mlx-vlm.py, wskażemy najlepsze otwarto-źródłowe repozytoria z przykładami, omówimy strategie optymalizacji (takie jak kwantyzacja czy grupowanie wag) oraz przedstawimy dobre praktyki i typowe pułapki. Na koniec sformułujemy wnioski i rekomendacje dotyczące pracy z modelami MLX.

Przegląd technik konwersji dla NLP, wizji komputerowej i modeli głosowych

Konwersja modeli do formatu MLX zależy od rodzaju modelu i jego architektury. MLX udostępnia osobne pakiety i narzędzia dla modeli czysto tekstowych (LM) oraz dla modeli multimodalnych (VLM – Vision-Language Models). Poniżej przedstawiamy podejście do konwersji w trzech kategoriach: modele NLP (językowe), modele wizji komputerowej (obrazowe, multimodalne) oraz modele głosowe (mowy).

Konwersja modeli NLP (LLM) do MLX-LM

Dla modeli językowych (np. GPT, LLaMA, Mistral, itp.) przeznaczony jest moduł MLX-LM. W większości przypadków konwersja modeli NLP z Hugging Face (które często dostarczane są w formacie PyTorch safetensors) do MLX jest zautomatyzowana. MLX-LM potrafi pobrać model z Hub Hugging Face i przekonwertować go wewnętrznie na format zgodny z MLX. Sprowadza się to do przemapowania wag i konfiguracji modelu na strukturę oczekiwaną przez bibliotekę MLX oraz (opcjonalnie) wykonania kwantyzacji wag.

Najprostszym sposobem jest użycie polecenia CLI lub API Python dostarczanego przez mlx-lm. Przykładowo, aby przekonwertować model 7B Mistral na format MLX i jednocześnie go skwantyzować, można użyć polecenia z linii komend:

# Instalacja MLX-LM
pip install -U mlx-lm

# Konwersja modelu Mistral 7B do formatu MLX z kwantyzacją 4-bit
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.3 --quantize

Powyższa komenda pobierze model mistralai/Mistral-7B-Instruct-v0.3 z Hugging Face, przekształci go do formatu MLX i domyślnie zapisze w podkatalogu (np. mlx_model). Domyślne ustawienia --quantize zazwyczaj oznaczają kwantyzację 4-bitową, ale można też doprecyzować parametry (np. --q-bits 4 dla 4-bit, --q-group-size 128 itp.). Alternatywnie można skorzystać z API Python:

from mlx_lm import convert
repo = "mistralai/Mistral-7B-Instruct-v0.3"         # or local path to safetensors
convert(repo, quantize=True, q_bits=4, q_group_size=128, mlx_path="mistral-7B-mlx")

Takie wywołanie automatycznie kwantyzuje model do 4-bit (z grupowaniem co 128 elementów) i zapisuje wynik w folderze mistral-7B-mlx. Według dokumentacji, wynikowa konwersja zawiera model w formacie MLX (wciąż opartym o pliki safetensors, lecz z zestrukturyzowanymi wagami zgodnie z implementacją MLX) oraz pliki konfiguracyjne/tokenizera potrzebne do uruchomienia. Co istotne, MLX-LM implementuje leniwe wczytywanie modelu podczas konwersji – oznacza to, że nie musi trzymać całych wag modelu w pamięci na raz, co ułatwia konwersję bardzo dużych modeli na komputerach z ograniczoną pamięcią ￼. Dzięki temu możliwe jest konwertowanie np. 70-miliardowych modeli językowych na MacBookach z 32 GB RAM, podczas gdy standardowe podejście wymagałoby załadowania ~120 GB danych jednocześnie (co jest niewykonalne) ￼.

Po konwersji model MLX-LM można uruchomić za pomocą poleceń generowania tekstu dostarczanych przez MLX. Przykładowo, aby wygenerować tekst z przekonwertowanego modelu, wystarczy:

mlx_lm.generate --model mlx-community/My-Mistral-7B-Instruct-v0.3-4bit --prompt "Hello, how are you?"

Zakładając, że model został wcześniej przesłany do Hugging Face Hub (np. do repozytorium mlx-community), MLX-LM automatycznie go pobierze i uruchomi. Można również wskazać lokalną ścieżkę do modelu przekonwertowanego (np. katalog mlx_model).

Podsumowując, dla modeli NLP proces konwersji jest w dużej mierze zautomatyzowany przez narzędzia MLX-LM. Należy upewnić się, że oryginalny model jest dostępny w formacie safetensors (w przeciwnym razie warto go najpierw przekonwertować, np. za pomocą skryptów Hugging Face, gdyż MLX-LM preferuje safetensors). Konwersja obejmuje przeniesienie wag do struktury MLX oraz ewentualną kwantyzację celem zmniejszenia rozmiaru modelu.

Konwersja modeli wizji komputerowej (multimodalnych) do MLX-VLM

Modele łączące obraz z językiem naturalnym – czyli VLM (Vision-Language Models) – wymagają nieco innego podejścia. Przykładami takich modeli są LLaVA, MiniGPT-4, InternVL, Idefics, Qwen-VL czy Gemma. Do ich obsługi służy paczka MLX-VLM, rozwijana społecznościowo (m.in. przez autora o pseudonimie Blaizzy). MLX-VLM umożliwia inferencję i drobne trenowanie modeli multimodalnych na Macach z Apple Silicon, wykorzystując wewnętrznie bibliotekę MLX. Konwersja modeli VLM często polega na podobnych krokach jak dla LLM, lecz z uwzględnieniem dodatkowego komponentu wizji (sieci przetwarzającej obraz).

W praktyce, narzędzie mlx_vlm.convert (analogiczne do powyższego) potrafi przekonwertować model multimodalny z Hugging Face do formatu MLX. Na przykład dla modelu Qwen2-VL 72B możemy spróbować komendy:

pip install -U mlx-vlm

# Próba konwersji Qwen2-VL-72B (może wymagać dużo pamięci)
python -m mlx_vlm.convert --hf-path Qwen/Qwen2-VL-72B-Instruct -q

W powyższym przykładzie przełącznik -q oznacza kwantyzację (domyślnie 4-bitową). Warto zauważyć, że konwersja tak ogromnego modelu może wymagać znaczących zasobów – obecna implementacja MLX-VLM nie jest tak zoptymalizowana pod względem użycia pamięci jak MLX-LM ￼. Według relacji użytkowników, mlx_vlm.convert wczytuje cały model do RAM przed konwersją, co przy modelu 72B (~120GB w 16-bit) jest praktycznie niemożliwe na typowym sprzęcie ￼. Autor projektu MLX-VLM aktywnie rozwija to narzędzie, więc w przypadku napotkania błędów dla nietypowych modeli (np. InternVL wcześniej nie był wspierany, co zgłoszono w issues ￼ ￼) warto sprawdzić otwarte zgłoszenia lub zaktualizować pakiet do najnowszej wersji. Często wsparcie dla nowych architektur jest dodawane przez społeczność (np. obsługa InternVL została dodana w ramach poprawek).

Konwertując model wizualno-językowy, należy upewnić się, że posiadamy wszystkie wymagane pliki modelu z Hugging Face: zwykle config.json (konfiguracja modelu tekstowego i wizualnego), tokenizer.json i ewentualnie processor do obrazów (np. konfiguracja Vision Encoder, często w pliku preprocessor_config.json lub w config). Wagi modelu mogą być dostarczone jako jeden plik .safetensors zawierający oba komponenty (z rozróżnieniem kluczy warstw tekstowych i wizualnych), albo oddzielnie. Typowy wzorzec dla modeli VLM na Hugging Face to właśnie pojedynczy plik safetensors wraz z odpowiednimi plikami konfiguracyjnymi ￼ ￼. Konwerter MLX-VLM mapuje wagi tak, by pasowały do wewnętrznych klas modeli MLX. Jeśli model nie jest wspierany natywnie, może być konieczne napisanie obsługi architektury – np. definiując klasy modelu w MLX odpowiadające oryginalnemu modelowi i wskazując, jak przenieść parametry.

Po pomyślnej konwersji model VLM również można uruchomić za pomocą interfejsu MLX. MLX-VLM dostarcza komendę generacji, gdzie podajemy zarówno ścieżkę do obrazu jak i prompt tekstowy. Dla przykładu, mając przekonwertowany model Gemma-3-27B-PT (4-bit) w repozytorium mlx-community, można wywołać:

python -m mlx_vlm.generate --model mlx-community/gemma-3-27b-pt-4bit \
    --prompt "Opisz co znajduje się na obrazku." --image input.jpg --max-tokens 100

Taka komenda załaduje model Gemma w formacie MLX (skonwertowany uprzednio z oryginalnego google/gemma-3-27b-pt ￼), przetworzy obraz input.jpg i wygeneruje odpowiedź tekstową (opis obrazu). Możliwe jest także uruchomienie trybu rozmowy z interfejsem graficznym (Gradio) poleceniem python -m mlx_vlm.chat_ui --model <ścieżka>, co uruchamia lokalny chat z możliwością podawania obrazów ￼ ￼.

Podsumowując, konwersja modeli wizji+języka wymaga obsługi zarówno części wizualnej jak i tekstowej. MLX-VLM wspiera wiele popularnych architektur (jak LLaVA, Idefics 2, Qwen2-VL, Phi-3, Pix2Pix Captioning itp. – dokumentacja wymienia wspierane modele ￼). Dla mniej popularnych modeli może być konieczne poczekanie na wsparcie lub samodzielne dostosowanie skryptów konwersji. W każdym przypadku należy przygotować odpowiednie pliki wejściowe (wagi safetensors, configi, tokenizery) i być świadomym wymagań pamięci podczas konwersji.

Konwersja modeli głosowych (ASR/TTS) do formatu MLX

Trzecią kategorią są modele głosowe, np. modele rozpoznawania mowy (ASR) czy syntezy mowy (TTS). Najbardziej znanym otwartym modelom ASR jest Whisper od OpenAI. Apple udostępniło dedykowaną paczkę mlx-whisper, która zawiera zaimplementowane modele Whisper w oparciu o MLX ￼. Zasadniczo więc konwersja modeli mowy może nie wymagać ręcznego przenoszenia wag – społeczność MLX już przygotowała wersje modeli Whisper w formacie MLX (dostępne na Hugging Face pod nazwami typu mlx-community/whisper-medium-mlx, itp.). Przykładowo, model whisper-medium-mlx jest opisany jako konwersja oryginalnego whisper-medium do formatu MLX ￼.

Jeśli jednak dysponujemy własnym modelem mowy w formacie safetensors (np. zmodyfikowany Whisper lub inny model TTS), proces konwersji wymaga zapewne dostosowania architektury w MLX podobnie jak w przypadku VLM. Modele ASR są zwykle enkoder-dekoderami transformers, co oznacza, że MLX-LM (skoncentrowane głównie na dekoderach auto-regresywnych) może nie obsługiwać ich natywnie. W takiej sytuacji dostępne są dwie drogi:
	•	Skorzystać z istniejących implementacji (jak wspomniany mlx-whisper) i ewentualnie wczytać wagi naszego modelu zamiast predefiniowanych.
	•	Lub przetransferować model do formatu Core ML za pomocą narzędzi Apple (np. coremltools), choć to podejście wykracza poza MLX i oznacza tworzenie statycznego modelu CoreML.

W przypadku mlx-whisper, korzystanie z modeli jest bardzo proste – pakiet ten udostępnia API analogiczne do oryginalnego OpenAI Whisper. Przykładowo, aby rozpoznać mowę z pliku audio za pomocą MLX Whisper, wystarczy kilka linijek w Pythonie:

import mlx_whisper
model = mlx_whisper.load_model("medium")  # załaduje whisper-medium w formacie MLX
result = model.transcribe("speech.wav")
print(result["text"])

Konwersja własnego modelu głosowego do MLX może wymagać zagłębienia się w implementację MLX i potencjalnie napisania własnego skryptu mapującego wagi (podobnie jak robi to convert_mlx-vlm.py dla modeli multimodalnych). Należy pamiętać, że warunkiem wstępnym jest posiadanie wag w formacie, który da się załadować bezpiecznie (preferowane safetensors). Przykładowo, wspomniany wcześniej użytkownik próbujący konwertować niestandardowy model Whisper odnotował błąd “No safetensors found” ￼ – co wskazuje, że model nie miał dostępnych wag w formacie safetensors w hubie HF. W takiej sytuacji należy najpierw przekonwertować oryginalne pliki (np. .pt Pytorcha) na .safetensors.

Reasumując, modele głosowe mogą być obsłużone w MLX dzięki dedykowanym bibliotekom (mlx-whisper dla ASR). Konwersja innych modeli mowy jest możliwa, ale może wymagać większego nakładu pracy (brak gotowych narzędzi tak wygodnych jak mlx-lm/vlm). W razie potrzeby, wzorując się na implementacji Whisper w MLX, można opracować analogiczne wsparcie dla innego modelu.

Analiza narzędzi do konwersji i porównanie z innymi formatami (PyTorch, TensorFlow, JAX)

Ekosystem MLX nie jest jedynym sposobem uruchamiania modeli – dlatego warto porównać go z tradycyjnymi rozwiązaniami i formatami, takimi jak standardowe modele PyTorch, TensorFlow czy JAX/Flax, a także innymi formatami wymiany modeli. Poniżej dokonujemy analizy narzędzi konwersji oraz formatu MLX na tle konkurencyjnych podejść.

Narzędzia do konwersji w MLX: Jak wspomniano, główne narzędzia to:
	•	mlx-lm – obsługuje konwersję modeli językowych. Zapewnia zarówno CLI (mlx_lm.convert) jak i API Python (mlx_lm.convert(...)), a także funkcje do generacji i serwowania modeli ￼ ￼. Umożliwia też automatyczne uploadowanie skonwertowanych modeli do Hugging Face (np. parametrem upload_repo) ￼.
	•	mlx-vlm – podobnie, dla modeli multimodalnych; udostępnia CLI (mlx_vlm.convert) oraz interfejs inference (mlx_vlm.generate, UI chat itp.) ￼ ￼. W odróżnieniu od mlx-lm, jest to projekt community-driven, przez co bywa mniej dopracowany w kwestii wydajności (np. brak pełnego lazy loading przy konwersji dużych modeli, co omawialiśmy) ￼.

Format wynikowy MLX vs formaty PyTorch/TF/JAX:
Modele MLX po konwersji wciąż korzystają z plików safetensors do przechowywania wag, jednak różnią się strukturą plików i oczekiwaniami biblioteki runtime:
	•	Typowy model PyTorch na Hugging Face składa się z config.json, tokenizer.json, itp. oraz jednego lub kilku plików .bin lub .safetensors z wagami. Wagi mogą zawierać dowolny kod (w przypadku .bin), co rodzi ryzyko bezpieczeństwa. Safetensors eliminują ten problem, zapewniając, że plik zawiera tylko surowe tensory ￼. PyTorch ładuje model poprzez zrekonstruowanie obiektów modelu i wczytanie tensorków do struktury modelu.
	•	TensorFlow tradycyjnie używa formatu SavedModel (folder z protobuferami .pb) lub dawniej .h5. Te formaty są również bezpieczne (dane to graf i tensory), ale mniej popularne w przypadku nowych LLM – społeczność najczęściej skupia się na PyTorch. Konwersja z PyTorch do TF lub odwrotnie jest możliwa poprzez wspólne formaty (np. ONNX, safetensors lub przy wsparciu Hugging Face Transformers dla modeli wspieranych jednocześnie w Torch i TF). Ogólnie jednak TensorFlow rzadko jest wykorzystywane do LLM na Mac (brak specjalnej optymalizacji pod M1/M2).
	•	JAX/Flax używa własnego formatu np. plików .msgpack + zbiór binarnych plików zawierających numpy array (Hugging Face pakuje to czasem w safetensors również). Flax modele również nie wykonują kodu przy ładowaniu – podobnie jak TF, dostarczają tylko dane. Konwersja między Flax a PyTorch modelami bywa możliwa jeśli istnieje odwzorowanie architektur (Hugging Face udostępnia często zarówno wersję PyTorch jak i Flax danego modelu). JAX jednak nie ma natywnej obsługi GPU MPS, więc na Macach modele Flax działają CPU-owo, co jest mało wydajne.
	•	MLX format można traktować jako specjalizowany wariant formatu PyTorch safetensors, przystosowany do silnika MLX. Po konwersji zazwyczaj zachowana jest struktura plików podobna do Hugging Face (config, tokenizer, model.safetensors). Różnica tkwi w tym, że klucze tensorów i config mogą ulec zmianie (np. inne nazwy warstw, zintegrowane warstwy Q/K/V atd.) aby odpowiadać implementacji modelu w MLX. Ponadto, jeśli zastosowano kwantyzację, część wag jest zapisana w niższej precyzji lub w formie grup skompresowanych. Dla przykładu, model Gemma-3-27B-PT przekonwertowany do MLX 4-bit ma rozmiar pliku ~5GB i parametry typu FP16/U32 ￼ – co sugeruje przechowywanie skal w FP16 i wartości kwantyzowanych jako int (32-bit z upakowanymi 4-bitowymi wartościami). Sam MLX w trakcie inferencji używa tych danych z maksymalnym wykorzystaniem sprzętu Apple. W efekcie, na M3 Max Apple, modele uruchomione przez MLX osiągają znakomite wyniki wydajności (np. ~250 tokenów/s dla modelu 1B LLaMA 3.2) ￼, co przewyższa wydajność jaką uzyskalibyśmy w czystym PyTorch na tym samym sprzęcie.

Warto porównać również uniwersalność formatów:
	•	Modele MLX są przeznaczone głównie na platformę Apple (macOS, iOS). Zaletą jest optymalizacja, wadą – mniejsza przenośność. Dla kontrastu, formaty jak ONNX czy TorchScript mogą działać wszędzie, ale nie będą tak wydajne na M1/M2 jak natywny MLX.
	•	Safetensors jako format pośredni jest wspierany właściwie wszędzie (PyTorch, TF, JAX, NumPy, Paddle) ￼. MLX również korzysta z safetensors, więc w pewnym sensie konwersja do MLX nie zmienia samego nośnika wag, a tylko ich ułożenie i typy.
	•	Jeśli chodzi o narzędzia konwersji innych niż MLX: istnieje np. transformers.convert_graph (Hugging Face) do ONNX, narzędzia do konwersji modelu na CoreML (dla iOS) poprzez coremltools, czy eksport do formatów runtime jak ggml/gguf (używane w llama.cpp). Ciekawostką jest, że pojawiają się ścieżki migracji między MLX a innymi formatami – np. najnowsze wersje MLX-examples potrafią “fuzjować model do formatu HF i konwertować do GGUF” ￼, co oznacza możliwość wyeksportowania modelu MLX do standardowego formatu Hugging Face, a stamtąd np. do biblioteki obsługującej CPU.

Poniższa tabela zbiera krótkie porównanie formatów i narzędzi:

Framework/Format	Plik wag	Bezpieczeństwo	Wsparcie kwantyzacji	Wsparcie Apple Silicon
PyTorch (HF)	.bin lub .safetensors	.bin podatny (pickle), safetensors bezpieczny ￼	Brak natywnego 4-bit (wymaga bibliotek zewn.)	MPS (działa, ale brak optymalizacji int4)
TensorFlow	SavedModel (.pb), .h5	Bezpieczny (dane + graf)	Int8 TFLite (dla mniejszych modeli)	Brak dedyk. optymalizacji LLM na M1/M2
JAX/Flax	.msgpack + shardy (czasem safetensors)	Bezpieczny	Brak (ew. offline quant)	Ograniczone (CPU lub eksperymentalnie)
MLX-LM/VLM (Apple)	.safetensors (po konwersji)	Bezpieczny (safetensors) ￼	Tak – 4-bit, 8-bit wbudowane (grupowanie)	Pełne (wykorzystanie GPU/ANE, wysokie token/s)

Legenda: ANE = Apple Neural Engine, HF = Hugging Face Hub.

Z powyższego wynika, że MLX stanowi atrakcyjną opcję dla użytkowników sprzętu Apple chcących korzystać z dużych modeli lokalnie. Łączy bezpieczeństwo formatu safetensors z optymalizacją hardware’ową. W porównaniu z użyciem surowego PyTorch, MLX często oferuje znacznie lepszą wydajność (dzięki kwantyzacji i lepszemu wykorzystaniu układów). Natomiast minusem jest konieczność dokonania konwersji modeli oraz pewne ograniczenia co do obsługiwanych architektur (PyTorch jest bardziej uniwersalny – można zawsze zaimplementować niestandardową architekturę bezpośrednio, zaś w MLX trzeba poczekać na wsparcie lub je dodać).

Sprawdzenie poprawności implementacji Gemma-3-27B-PT w convert_mlx-vlm.py

Jako studium przypadku weźmy konwersję modelu Gemma-3-27B-PT. Gemma 3 to rodzina modeli multimodalnych od Google, gdzie wersja 27B posiada zdolność rozumienia obrazów (Vision + Language). Oryginalny model google/gemma-3-27b-pt jest dostępny w hubie Hugging Face (prawdopodobnie w formacie safetensors ze względu na rozmiar). Istnieje już skonwertowana wersja 4-bit tego modelu w repozytorium mlx-community/gemma-3-27b-pt-4bit, przygotowana narzędziem MLX-VLM v0.1.17 ￼. Jednak załóżmy, że chcemy samodzielnie przekonwertować ten model i upewnić się, że implementacja skryptu convert_mlx-vlm.py jest poprawna.

Etapy implementacji konwersji Gemma-3-27B-PT:
	1.	Wczytanie oryginalnego modelu: Skrypt powinien załadować plik safetensors z wagami Gemma 27B oraz konfigurację modelu. Jeżeli model jest podzielony na shardy (ze względu na rozmiar, np. kilka plików safetensors), należy wszystkie je wczytać. Przy braku natywnej obsługi Gemma w MLX-VLM, skrypt musi znać strukturę modelu (np. czy jest to architektura podobna do PaLM lub LLaVA).
	2.	Mapowanie warstw: Kluczowa część to mapowanie nazw warstw i kształtów z oryginalnego modelu na odpowiedniki w MLX. Trzeba zweryfikować, że wszystkie wymagane podmoduły (część tekstowa i wizualna) są odwzorowane. Dla Gemma-3-27B-PT część “PT” może sugerować model tekstowy pretrained, a do tego moduł wizji. Implementacja powinna utworzyć np. instancję modelu MLX składającą się z encodera wizji i dekodera językowego, i przypisać do nich wagi.
	3.	Kwantyzacja (opcjonalna): Jeśli konwersja docelowo ma być 4-bit (zgodnie z celem, by zmieścić model w ~5GB), skrypt może zaaplikować kwantyzację. W convert_mlx-vlm.py warto sprawdzić, czy korzysta on z metod MLX do kwantyzacji, czy robi to “ręcznie”. MLX-VLM być może udostępnia funkcję do kwantyzacji podobnie jak MLX-LM.
	4.	Zapis modelu: Po odwzorowaniu wag, skrypt powinien zapisać nowy plik safetensors (lub kilka plików) zawierający już wagi w formacie MLX. Do tego zapewne generowany jest nowy config.json kompatybilny z MLX-VLM (z odpowiednim typem modelu, np. "model_type": "gemma3_text" lub podobnie, zależnie od implementacji MLX). Należy też skopiować oryginalne tokenizery i ewentualne inne pliki (np. merges.txt jeśli BPE).

Sprawdzenie poprawności:
Aby upewnić się, że implementacja jest poprawna, należy przetestować parę rzeczy:
	•	Integracja modelu: Spróbować załadować przekonwertowany model za pomocą MLX-VLM (np. mlx_vlm.load("ścieżka_do_modelu")) i sprawdzić, czy nie rzuca błędów o brakujących tensorach lub niespójnej konfiguracji. Jeśli model się ładuje, to struktura plików i nazwy warstw prawdopodobnie są zgodne.
	•	Wyniki na znanych danych: Najlepszym testem jest porównanie wyjść modelu MLX z wyjściami oryginalnego modelu (PyTorch) na tym samym wejściu. Ponieważ pełny model Gemma-27B jest ogromny, można użyć np. trybu zerowego strzału na prostym zadaniu:
	•	Wziąć przykładowy obraz i pytanie do niego, przepuścić przez oryginalny model (o ile to możliwe, np. w środowisku GPU z >40GB pamięci lub w trybie 8-bit w Transformers) i uzyskać wynik tekstowy.
	•	Ten sam obraz i pytanie podać do modelu MLX i porównać odpowiedź. Nie musi być bitowo identyczna (bo np. kwantyzacja 4-bit wprowadza aproksymację), ale powinna mieć logicznie zbliżoną treść. Jeśli MLX wygeneruje kompletnie nieadekwatny tekst lub same tokeny padding (<pad>...), to sygnał, że coś poszło nie tak. Przykładowo, błąd w kwantyzacji Gemma-2-27B ujawnił się tym, że model generował wyłącznie token <pad> ￼ – to wyraźny symptom niepoprawnej konwersji wag lub błędu w kwantyzacji. Poprawna implementacja po naprawie takiego błędu powinna generować sensowne zdania.
	•	Porównanie statystyk wag: Dodatkowo można sprawdzić czy podstawowe statystyki wag (np. sumy, rozkłady) mniej więcej się zgadzają przed i po konwersji (oczywiście przed kwantyzacją). Jeśli podczas konwersji 1:1 (bez kwantyzacji) suma wag warstwy self-attention w oryginalnym modelu różni się od sumy wag w MLX, może to wskazywać na błąd w transponowaniu lub pominięcie części wag.

Podsumowując, aby zweryfikować poprawność konwersji Gemma-3-27B-PT w skrypcie convert_mlx-vlm.py, należy przeprowadzić testy wczytania i działania modelu. Dobrą praktyką jest przygotowanie małego testu integracyjnego – np. użycie zredukowanej wersji modelu (gdyby istniała mniejsza Gemma 3 1B) na prostym wejściu i porównanie wyników z oryginałem. Jeśli wszystko się zgadza, możemy być pewni, że implementacja jest poprawna. W przeciwnym razie, debugowanie należy zacząć od sprawdzenia zgodności architektury (czy wszystkie bloki modelu zostały zamapowane) oraz poprawności kwantyzacji.

Najlepsze otwarto-źródłowe repozytoria i przykłady implementacji

Ekosystem MLX rozwija się dynamicznie, a społeczność open-source udostępnia wiele pomocnych zasobów. Poniżej zestawiono godne polecenia repozytoria i przykłady, które mogą ułatwić proces konwersji i uruchamiania modeli w MLX:
	•	Oficjalne repozytorium Apple MLX – dostępne na GitHubie: apple/mlx oraz repozytorium z przykładami ml-explore/mlx-examples. Zawiera rdzeń biblioteki (C++ oraz frontendy Python/Swift ￼) oraz przykłady użycia m.in. do LLM. W folderze llms/ znajdują się skrypty konwersji, quantyzacji, fine-tuningu itp. ￼ ￼.
	•	MLX Community na Hugging Face – profil mlx-community na HF Hub zawiera dziesiątki (jeśli nie setki) przekonwertowanych modeli, zarówno LLM jak i VLM. Są tam m.in. modele Mistral, LLaMA 2, Qwen, IdeFICS, Gemma, Whisper i inne, często w wariantach 4-bit. Każdy model ma kartę z informacją, z jakiego oryginału został przekonwertowany i jaką wersją narzędzia (np. “mlx-vlm version 0.1.17”) ￼. To kopalnia gotowych przykładów – można podejrzeć pliki w sekcji “Files and versions”, aby zobaczyć jak wygląda struktura przekonwertowanego modelu (np. jakie pliki safetensors, configi).
	•	Repozytorium MLX-VLM – Blaizzy/mlx-vlm (oraz fork ml-explore/mlx-vlm) – zawiera kod źródłowy pakietu MLX-VLM. Można tam znaleźć definicje obsługiwanych modeli wizualnych (np. LLaVA, Qwen-VL etc.) i skorzystać z tego jako wzorca dodając nowy model. W dokumentacji wymienione są też modele obsługujące multi-image input, itp. ￼. To repo jest świetnym źródłem, jeśli planujemy dodać wsparcie dla kolejnego modelu – możemy zobaczyć, jak zaimplementowano inne.
	•	Przykłady w Swift – ml-explore/mlx-swift-examples. Zawiera porty modeli MLX do Swifta (na potrzeby iOS/macOS natywnie). W szczególności część MLXVLM pokazuje jak definiować konfigurację modeli multimodalnych w Swift i ładować wagi safetensors ￼. To ciekawe źródło, bo przedstawia alternatywne podejście do integracji – jeśli ktoś chce np. zaimplementować model w aplikacji iOS, może skorzystać z tych przykładów.
	•	Repozytoria projektów portujących modele – np. JosefAlbers/Phi-3-Vision-MLX który zawiera kod i instrukcje portowania modelu Phi-3 (multimodalnego) do MLX. Autor opisał swoje doświadczenia również w serii artykułów (Medium/Dev.to) omawiających szczegóły implementacji i optymalizacji takiego modelu. Dzięki temu repo można prześledzić realny proces konwersji modelu od podstaw (w tym implementację niestandardowych rozwiązań jak Su-scaled RoPE dla 128k kontekstu).
	•	Blogi i poradniki – pojawia się coraz więcej wpisów na blogach opisujących użycie MLX. Warto wymienić:
	•	Post na Medium: “Running Small Vision Language Models (VLMs) on MLX” – opisujący jak uruchomić małe modele VLM na Macu ￼.
	•	Wpis na blogu LM Studio o integracji MLX: “LM Studio 0.3.4 ships with Apple MLX” – zawiera przegląd tego, co MLX oferuje i jak to zostało wbudowane w aplikację LM Studio ￼. Pozwala zrozumieć kontekst użycia MLX w praktycznym narzędziu.
	•	Poradnik “Fine-tuning LLMs with LoRA and MLX-LM” autorstwa Joany Levtcheva – opisujący, jak wykorzystać MLX-LM do fine-tuningu modeli z Low-Rank Adaptation ￼.
	•	Blog Niklasa Heidloffa o lokalnym fine-tuningu na MLX ￼ – przeprowadza przez proces trenowania 7B modelu na MacBooku M3 z użyciem MLX.
	•	Wpis na blogu StrathWeb: “Fine tuning Phi models with MLX” – praktyczny przykład fine-tuningu modelu multimodalnego (Phi-3) z użyciem MLX ￼.
	•	Simon Willison również eksperymentował z MLX (np. “Run LLMs on macOS using llm-mlx” ￼ oraz “mlx-whisper” ￼), co dostarcza dodatkowych wskazówek i narzędzi (Simon udostępnił np. plugin do własnej biblioteki LLM umożliwiający korzystanie z MLX).

Dzięki powyższym repozytoriom i źródłom, deweloperzy mają solidną bazę do nauki. Zalecane jest przejrzenie kodu konwertujących modele podobne do tego, który nas interesuje – np. jeśli konwertujemy nowy model wizualny, sprawdźmy jak zrobiono to dla LLaVA lub Qwen-VL. Społeczność MLX jest aktywna na forum Hugging Face i Reddit (np. w wątku r/LocalLLaMA pojawia się wiele dyskusji o MLX), więc warto tam szukać porad i ewentualnie zadawać pytania.

Strategie optymalizacji modeli w MLX (kwantyzacja, grupowanie wag, precyzja obliczeniowa)

Jednym z głównych powodów konwersji modeli do formatu MLX jest możliwość ich dalszej optymalizacji pod kątem wydajności i zużycia pamięci na sprzęcie Apple. Omówmy kluczowe strategie optymalizacji dostępne w MLX:

1. Kwantyzacja wag:
Kwantyzacja to metoda redukcji precyzji wag (np. z 16-bit do 4-bit), co drastycznie zmniejsza rozmiar modelu w pamięci i często pozwala przyspieszyć inferencję (dzięki mniejszej ilości danych do przetwarzania, a w przypadku Apple Neural Engine – dzięki natywnemu wsparciu dla operacji na int8). MLX-LM i MLX-VLM mają wbudowaną obsługę kwantyzacji podczas konwersji. Domyślnie używana jest kwantyzacja 4-bitowa (na wzór GPTQ). Jak to działa? Zazwyczaj wagi każdej warstwy są grupowane w bloki (np. po 32 lub 128 wartości – parametry group size) i w obrębie takiego bloku są skalowane do 4-bitowych wartości plus jedna wspólna skala (stąd potrzeba przechowywania np. FP16 skali). Taki schemat pozwala zachować rozsądną dokładność przy dużej redukcji rozmiaru.

W MLX mamy możliwość dostosowania kwantyzacji poprzez parametry:
	•	q_bits – liczba bitów (4 lub 8 obecnie). 8-bit daje mniejszą kompresję, ale wyższą wierność oryginałowi.
	•	q_group_size – rozmiar grupy (np. 32, 64, 128). Mniejsza grupa oznacza więcej skal (czyli więcej metadanych, nieco większy model) ale dokładniejsze dopasowanie rozkładu wag; większa grupa – większa kompresja kosztem dokładności.
	•	quantize_embeddings lub podobne opcje – czy kwantyzować również warstwy embeddingów i outputowe. Czasem pozostawia się te krytyczne warstwy w wyższej precyzji.

Dobra praktyka to zacząć od standardowych ustawień (4-bit, grupy 32 lub 128 – MLX domyślnie zdaje się używać 32 ￼ lub 128). Po kwantyzacji 4-bit model zajmuje ~25% oryginalnego rozmiaru (FP16). W przypadku Gemma 27B np. z ~20GB FP16 do ~5GB 4-bit. Wydajność również rośnie: modele 4-bit mogą korzystać z obliczeń INT4/INT8 na dedykowanych jednostkach (np. AMX w Apple Silicon) – stąd imponujące szybkości generacji tekstu. Należy jednak uważać: nadmierna kwantyzacja (np. do 3-bit, gdyby była wspierana, lub niekorzystne skalowanie) może zniszczyć zdolności modelu. Zazwyczaj 4-bit zachowuje ~95% jakości oryginału, 8-bit praktycznie pełną jakość.

2. Grupowanie i fuzja wag:
Grupowanie wag odnosi się przede wszystkim do wspomnianego mechanizmu blokowego w kwantyzacji, ale można też mówić o fuzji wag (weight tying / sharing) czy optymalizacji struktury modelu. MLX przy konwersji może wykonywać pewne fuzje warstw – np. łączyć matryce Q, K, V atencji w jedną dużą macierz aby przyspieszyć obliczenia (to zależy od implementacji modelu w MLX). Dodatkowo, MLX-examples wspomina o możliwości “fuse the model to HF format” – czyli scalenia fragmentów modelu dla łatwiejszego eksportu ￼. Chodzi tu raczej o zagadnienia przy trenowaniu (fuzja LoRA z modelem bazowym itp.) niż o samą inferencję. Niemniej jednak, podczas konwersji warto upewnić się, czy np. warstwa normalizacji plus skalowanie jest w MLX realizowana jednym kernelem – takie niskopoziomowe detale są zarządzane przez bibliotekę, użytkownik nie musi ich ręcznie ustawiać. Ważne, aby nie pomieszać wag – każda warstwa w MLX musi dostać odpowiadający jej fragment wag.

3. Precyzja obliczeniowa (dtype):
MLX domyślnie operuje na 16-bitowej precyzji FP16 dla obliczeń zmiennoprzecinkowych, co jest natywne dla GPU Apple (M1/M2 mają wektorowe jednostki 16-bit). Dla kwantyzowanych modeli wykorzystuje odpowiednio INT8/INT4 w segmentach, ale często z akumulacją w FP16. Użytkownik nie musi tego ręcznie konfigurować – wystarczy, że wskazał przy konwersji odpowiedni tryb. Gdyby ktoś chciał zachować pełną precyzję, mógłby przekonwertować model bez opcji --quantize. Wówczas w MLX model będzie w FP16 (bo MLX chyba konwertuje wszystko do FP16 podczas wczytywania, o ile model był w FP32 to i tak zrzuci do 16). Taki model FP16 zajmie więcej RAM, ale może minimalnie poprawić dokładność. W praktyce jednak 4-bitowe modele są najbardziej rozpowszechnione ze względu na korzystny kompromis. Można ewentualnie rozważyć 8-bit jako alternatywę – MLX-LM wspiera 8-bit quant (parametr --q-bits 8). 8-bit z grupami 32 to wciąż 50% rozmiaru FP16, a błąd kwantyzacji jest pomijalny; bywa używany dla bardzo czułych zadań lub gdy model już w 4-bit wykazywał degradację.

4. Optymalizacje specyficzne dla sprzętu Apple:
Apple Silicon oferuje nie tylko GPU z Metal Performance Shaders, ale i Neural Engine (ANE) oraz wektorowe jednostki AMX. MLX stara się wykorzystywać optymalną kombinację. Według twórców MLX, modele LLM mogą korzystać z ANE do pewnych operacji, co znacznie odciąża GPU przy generowaniu wielu tokenów. To jednak dzieje się automatycznie. Dla użytkownika ważne jest, że konwersja do MLX otwiera drogę do tych optymalizacji – coś, czego zwykły PyTorch nie zrobi (PyTorch na Mac korzysta tylko z GPU i CPU). Stąd w testach MLX generuje tekst szybciej niż np. Transformers na MPS. Wspomniany wynik ~250 tokenów/s na M3 Max dla modelu 1B ￼ pokazuje skok wydajności dzięki dostrojeniu do sprzętu. Inna optymalizacja to streamowanie generacji – MLX-LM wspiera streamowanie tokenów (generowanie strumieniowe) out-of-the-box ￼, co poprawia latency pierwszych tokenów.

Podsumowując, strategie optymalizacji modeli w MLX koncentrują się głównie na redukcji bitowej wag (kwantyzacja) oraz wykorzystaniu sprzętowych możliwości Apple Silicon. Przy konwersji warto domyślnie stosować kwantyzację 4-bit dla dużych modeli – umożliwia to uruchomienie modeli 30B na MacBooku z 16 GB pamięci, co inaczej byłoby niewykonalne. Należy dobrać odpowiedni rozmiar grupy i ewentualnie przetestować jakość modelu po kwantyzacji (jeśli znacząco spadnie, można spróbować 8-bit). Warto śledzić rozwój MLX, bo możliwe, że pojawią się kolejne techniki (np. pruning – przycinanie wag zerowych, czy distillacja modeli specjalnie pod MLX). Już teraz widać, że MLX staje się platformą do eksperymentów – np. integruje się z LoRA, umożliwia łączenie kilku modeli jednocześnie w pamięci (co pokazano w LM Studio, gdzie równolegle można ładować modele MLX i llama.cpp) ￼. Daje to ciekawą przestrzeń do optymalizacji na poziomie aplikacji (np. przełączanie między modelami zależnie od zadania).

Best practices i pułapki w konwersji

Konwertując modele do formatu MLX, warto kierować się kilkoma najlepszymi praktykami, aby proces przebiegł sprawnie i aby uniknąć typowych problemów. Oto zestaw rad oraz ostrzeżenie przed potencjalnymi pułapkami:

Najlepsze praktyki:
	•	Używaj najnowszych wersji narzędzi MLX: Zarówno mlx-lm jak i mlx-vlm są intensywnie rozwijane. Upewnij się, że masz zainstalowaną aktualną wersję (pip install -U mlx-lm mlx-vlm). Starsze wersje mogą zawierać bugi – przykładowo, we wcześniejszej wersji MLX-LM występował błąd powodujący generowanie samych <pad> po kwantyzacji Gemma-2, co zostało naprawione w nowszej aktualizacji ￼ ￼. Aktualizacja naprawiła problem, więc dbaj o bieżące łatki.
	•	Zawsze preferuj format safetensors jako źródło: Jeśli model źródłowy jest tylko w pliku .bin (pickle), rozważ konwersję do safetensors przed użyciem narzędzi MLX. Pozwoli to uniknąć błędów ładowania na maszynach bez CUDA (PyTorch przy próbie wczytania pickle może oczekiwać obecności GPU lub określonego device, jak widzieliśmy przy błędzie Attempting to deserialize object on a CUDA device... ￼). Safetensors wczyta się bez takich kłopotów.
	•	Sprawdzaj wsparcie architektury w MLX: Przed konwersją egzotycznego modelu, sprawdź w kodzie MLX-LM/VLM czy jest zaimplementowana jego architektura. Lista wspieranych modeli jest ograniczona do najpopularniejszych. Jeśli Twój model to wariant rodziny GPT/LLama (dekoder causal LM) – prawdopodobnie zadziała od razu. Jeśli to niestandardowy enkoder-dekoder lub model multimodalny – może wymagać wsparcia. Czasem włączenie flagi --trust-remote-code w MLX-LM potrafi dynamicznie załadować niestandardową architekturę (podobnie jak w Transformers), ale to działa tylko przy generowaniu, niekoniecznie przy konwersji. W razie braku wsparcia, rozważ zgłoszenie issue lub samodzielne dodanie kodu (bazując na podobnym modelu). Znak, że model nie jest wspierany, pojawi się jako błąd typu “Model type X not supported” ￼.
	•	Testuj model po konwersji na małych zadaniach: Zanim zaczniesz długą sesję z modelem, przetestuj go na prostym promptcie. Dla LLM – zapytaj o coś krótkiego; dla VLM – podaj prosty obraz i pytanie. Zweryfikuj, czy output wygląda sensownie. To szybki sposób wykrycia ewentualnego problemu (np. złej integracji tokenizera, co może objawić się bełkotem, albo błędu w warstwach – np. całkiem złe odpowiedzi lub brak reakcji).
	•	Monitoruj zużycie pamięci podczas konwersji: Zwłaszcza przy MLX-VLM, miej na uwadze RAM. Jeśli widzisz, że zużycie pamięci rośnie niepokojąco, a Twój model jest ogromny, istnieje ryzyko, że narzędzie się zawiesi lub ubije proces. W takiej sytuacji lepiej przerwać i spróbować innej metody (np. konwersja na maszynie z większą pamięcią lub poczekać na aktualizację z lazy loading). Ewentualnie, jeżeli model jest publicznie dostępny, sprawdź czy ktoś już go nie przekonwertował – być może unikniesz wysiłku (przykładowo, użytkownik próbował sam konwertować Qwen2-72B, podczas gdy autor MLX-VLM następnego dnia udostępnił gotowy model 4-bit w Hub ￼ ￼).
	•	Zachowaj oryginalne pliki konfiguracyjne/tokenizery: MLX często będzie ich potrzebować. Nawet jeśli skrypt konwertujący generuje nowy config, trzymaj pod ręką oryginał dla odniesienia. Często parametry jak liczba warstw, dim modelu, typ pozycyjnych embeddingów muszą być identyczne – oryginalny config.json jest tu wzorcem. Tokenizer natomiast zwykle jest używany bez zmian (MLX korzysta z Hugging Face tokenizers, więc pliki tokenizer.json, vocab.json itp. zadziałają).

Pułapki i problemy:
	•	Rozbieżności w tokenizacji i specjalne tokeny: Upewnij się, że model MLX używa tego samego tokenizera i specjalnych tokenów co oryginał. Czasem modele konwersacyjne wymagają dodania tokenów jak <bos> (beginning of sentence) czy sekwencji formatowania dla czatu. MLX-LM ma funkcje apply_chat_template ￼ ￼, które dbają o to formatowanie. Jeśli o tym zapomnisz, możesz uzyskać inne wyniki niż oczekiwane.
	•	Inny sposób generacji (sampler, temperature): Porównując wyniki przed i po, miej na uwadze, że MLX może mieć nieco inne domyślne hiperparametry generowania (np. max_new_tokens, temperatura). Staraj się trzymać podobnych ustawień. To drobnostka, ale ważna przy ocenie jakości.
	•	Potencjalne błędy liczbowe po kwantyzacji: Mimo że 4-bit z reguły działa dobrze, zdarzały się przypadki, że model tracił płynność generowania (zacinał się) lub miał powtarzalne wzorce błędów. Może to wynikać z charakterystyki modelu – np. modele z dużą czułością na wartości wag (wizyjne encodery) czasem gorzej znoszą kwantyzację. Jeśli zauważysz dziwne zachowanie, rozważ przejście na 8-bit dla części modelu. W MLX-VLM można np. nie kwantyzować części wizualnej, tylko tekstową.
	•	Długie czasy konwersji: Konwersja dużego modelu może potrwać. W trakcie nie ma zbyt wielu komunikatów (poza “Loading… Quantizing…”). Cierpliwość jest kluczem. Jeżeli proces trwa podejrzanie krótko i kończy się sukcesem, a model jest duży, upewnij się, że na pewno wszystkie pliki zostały wygenerowane. Może się zdarzyć, że np. część wag została pominięta (wtedy load modelu by się nie powiódł). Raczej jednak narzędzia MLX sygnalizują błędy jawnie.
	•	Incompatibility między wersjami MLX: Jeśli udostępniasz swój przekonwertowany model publicznie, zaznacz jaką wersją MLX-LM/VLM był konwertowany (tak jak robi to mlx-community w model card). Zdarzało się, że kolejna wersja MLX zmienia drobne rzeczy i modele z bardzo starych wersji wymagają ponownej konwersji. Trzymając informację o wersji, ułatwiasz innym decyzję czy ewentualnie przekonwertować model ponownie.

Stosując te zalecenia, znacznie zwiększasz szanse, że proces konwersji przebiegnie pomyślnie, a uzyskany model będzie działał zgodnie z oczekiwaniami. W skrócie: aktualizuj narzędzia, weryfikuj każdy krok i testuj wynik.

Wnioski i rekomendacje

Konwertowanie modeli z formatu safetensors do formatu MLX-LM/VLM otwiera nowe możliwości dla deweloperów i entuzjastów AI korzystających z komputerów Apple. Dzięki MLX można uruchamiać najnowsze modele językowe i multimodalne lokalnie, z wydajnością często przewyższającą tradycyjne rozwiązania oparte o PyTorch na Mac. Safetensors jako standard zapewnia bezpieczeństwo i interoperacyjność między frameworkami, a MLX dodaje do tego warstwę optymalizacji sprzętowej oraz wygodne interfejsy do inferencji i trenowania.

Z tego przewodnika wynika kilka kluczowych rekomendacji:
	•	Wykorzystuj MLX tam, gdzie to ma sens: Jeżeli celem jest uruchomienie modelu na MacBooku/iPhonie lub ogólnie na Apple Silicon – konwersja do MLX jest warta rozważenia. Zapewni to lepsze użycie dostępnych zasobów (GPU/ANE) niż standardowy PyTorch/TF.
	•	Stawiaj na kwantyzację 4-bit dla dużych modeli: Pozwala to zmieścić duże modele (13B, 30B, a nawet 70B) w pamięci i uzyskać zadowalające czasy odpowiedzi. Spadek jakości jest niewielki, a korzyści ogromne. W razie potrzeby zachowania pełnej dokładności – 8-bit również jest dobrą opcją.
	•	Korzystaj z narzędzi i wiedzy społeczności: Nie musisz wszystkiego robić od zera. Sprawdź czy Twój model nie został już przekonwertowany przez kogoś innego. Przeglądaj repozytoria mlx-community na HF, uczestnicz w dyskusjach. Zaoszczędzi to czas i uchroni przed znanymi problemami.
	•	Pamiętaj o testach porównawczych: Zawsze weryfikuj poprawność działania modelu MLX względem oryginału. Najlepiej przed i po konwersji przetestuj go na tym samym wejściu. To potwierdzi, że model zachował swoje możliwości (zwłaszcza ważne przy niestandardowych modelach lub agresywnej kwantyzacji).
	•	Bądź świadom ograniczeń: MLX stale się rozwija. Może nie obsługiwać pewnych nietypowych architektur lub funkcjonalności (np. bardzo specyficznych warstw). Planując projekt, miej plan awaryjny – np. jeśli MLX czegoś nie wspiera, czy możesz użyć fallback na PyTorch dla tego komponentu. Z czasem te luki są wypełniane, ale na moment pisania przewodnika wciąż jest to stosunkowo nowa technologia.

Na koniec, warto podkreślić, że Apple MLX reprezentuje szerszy trend – dostosowania narzędzi ML do specyfiki hardware (w tym przypadku, optymalizacja pod konkretną architekturę CPU/GPU). Podobne podejścia widzimy np. w bibliotekach takich jak NVIDIA TensorRT czy Intel Neural Compressor, jednak MLX wyróżnia się tym, że jest dostępne dla szerokiej społeczności użytkowników urządzeń konsumenckich (Mac). Konwersja modeli do MLX może stać się standardowym etapem w workflow developerów AI na Mac, podobnie jak konwersja do ONNX jest standardem dla wdrożeń chmurowych.

Rekomendacja ogólna brzmi: jeśli pracujesz z dużymi modelami i masz możliwość skorzystania z MLX – spróbuj tego. Doświadczenia wielu użytkowników pokazują, że modele działające w MLX potrafią zapewnić płynność działania niedostępną wcześniej na komputerach osobistych ￼. Ten przewodnik powinien ułatwić Ci pierwsze kroki w konwersji i optymalizacji modeli. Powodzenia w eksperymentach z MLX-LM/VLM i nie zapomnij dzielić się swoimi wynikami z społecznością!

Wprowadzenie: MLX (Machine Learning eXchange) to otwartoźródłowy framework od Apple ML Research, zaprojektowany do efektywnego trenowania i serwowania modeli AI na układach Apple Silicon ￼. Zawiera on dedykowane biblioteki do obsługi różnych typów modeli: MLX-LM dla dużych modeli językowych (NLP), MLX-image dla modeli wizji komputerowej, MLX-VLM dla modeli multimodalnych (wizja+język) oraz najnowsze MLX-audio dla modeli mowy. Modele zaadaptowane do MLX są zwykle przechowywane w formacie Safetensors z dołączonym plikiem konfiguracyjnym, co zapewnia szybkie i bezpieczne ładowanie wag. Format MLX zachowuje architekturę modelu (np. warstwy transformera LLaMA czy enkodera obrazu ViT) i parametry w układzie dostosowanym do MLX, podobnie jak PyTorch czy TensorFlow przechowują modele we własnych formatach (np. pliki .pt, SavedModel). Różnica polega na optymalizacji pod kątem sprzętu Apple – MLX korzysta z GPU Metal i ewentualnie Neural Engine, co przekłada się na dużą wydajność na Macach w porównaniu do uruchamiania modeli bezpośrednio w PyTorch na MPS ￼. W poniższym researchu omawiamy, jak przebiega konwersja różnych typów modeli do formatu MLX-LM/VLM, jakie narzędzia to ułatwiają, oraz jakie techniki optymalizacji (kwantyzacja, grupowanie wag, precyzja) są stosowane. Na końcu dokonujemy analizy bieżącej implementacji konwertera convert_mlx-vlm.py dla modelu Gemma-3-27B-PT pod kątem poprawności i zgodności z najlepszymi praktykami, wskazując ewentualne usprawnienia.

Konwersja modeli NLP do formatu MLX-LM

Obsługiwane architektury i narzędzia: MLX-LM (dostępny jako pakiet mlx-lm) zapewnia integrację z Hugging Face Transformers i obsługuje popularne architektury LLM, m.in. LLaMA, Mistral, Phi-2, Qwen ￼. Oznacza to, że modele takie jak LLaMA-2, Mistral 7B czy Google Gemma 3 mogą zostać wczytane i zapisane w formacie MLX bez ręcznej implementacji warstw, o ile ich architektura jest rozpoznawana przez MLX. W praktyce MLX-LM traktuje model Hugging Face jako bazę – pobiera konfigurację i wagi – po czym mapuje go na analogiczny model MLX. Wykorzystuje do tego własne klasy (np. warstwy Attention, FeedForward zaimplementowane w MLX) i zapisuje wagi w formacie Safetensors z meta-danymi. Sam proces jest zautomatyzowany przez narzędzie mlx_lm.convert, które można wywołać z CLI lub poprzez API Pythona ￼ ￼. Przykładowo, aby przekonwertować model Mistral 7B i od razu poddać go kwantyzacji, można użyć komendy: python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q ￼. Analogicznie w kodzie: from mlx_lm import convert; convert("mistralai/Mistral-7B-v0.1", quantize=True, upload_repo="user/Mistral-7B-4bit-MLX") – co pobierze model z HF Hub, skonwertuje go do MLX i wyśle do wskazanego repozytorium ￼. MLX-LM automatycznie dostosowuje typ danych (np. domyślnie FP16) i pozwala określić precyzję zapisu poprzez parametr --dtype (do wyboru float16, bfloat16, float32) ￼. W praktyce większość modeli NLP konwertuje się do FP16, co zapewnia balans między szybkością a zużyciem pamięci.

Technika konwersji: Pod maską, konwerter ładuje model za pomocą Hugging Face Transformers, następnie tworzy odpowiadającą mu instancję modelu MLX i kopiuje wagi do struktur MLX. Wagi zapisywane są jako plik .safetensors (lub kilka plików, jeśli model jest duży) wraz z plikiem indeksującym shardy. Dzięki wsparciu formatu safetensors i odpowiednim meta-danym, MLX wie jak interpretować tensory (np. czy model był zapisany w konwencji PT czy TF) ￼. Dla użytkownika konwersja jest niemal bezobsługowa – wystarczy podać ścieżkę do modelu na HF Hub lub lokalnie. Należy jednak upewnić się, że MLX-LM zna architekturę modelu. W przypadku nowych modeli może zajść potrzeba aktualizacji MLX. Przykładem jest Google Gemma 3 – najnowsza rodzina modeli językowych z elementami multimodalnymi. Gemma 3 ma architekturę zbliżoną do LLaMA/Mistral (jak zauważyli twórcy MLX, „Gemma is almost identical to a Mistral/Llama style model” ￼), ale w momencie udostępnienia wymagała dodania do katalogu architektur MLX. Rzeczywiście, zaraz po premierze Gemma 3 pojawił się Pull Request dodający wsparcie dla wariantu tekstowego (gemma3_text) w MLX-LM ￼. Dzięki temu duże modele Gemma 3 (1B, 4B, 12B, 27B parametrów) można już uruchamiać w MLX jako modele tekstowe. Jeśli jednak spróbujemy załadować model bez tej aktualizacji, MLX zgłosi błąd „Model type gemma3 not supported” ￼ ￼. Best practice: przed konwersją upewnij się, że masz najnowszą wersję mlx-lm – deweloperzy Apple i społeczność szybko dodają obsługę nowych modeli. Na szczęście MLX-LM jest stale rozwijany (np. wersja 0.21.6 wprowadziła wsparcie Gemma 3, mimo że na PyPI początkowo była 0.21.5 ￼).

Przebieg konwersji i weryfikacja: Konwertując model NLP, warto zwrócić uwagę na kilka aspektów: (1) Tokenizer i specyficzne pliki – MLX-LM zwykle sam pobiera tokenizer (np. tokenizer.json, vocab.json, merges.txt dla modeli BPE) i zapisuje go w formacie kompatybilnym. W przypadku modeli opartych o SentencePiece (LLaMA, Mistral) nie ma problemu, ale dla modeli z tokenizerem typu CLIP (patrz SigLIP niżej) brak pliku merges może powodować błędy ￼. Trzeba wtedy upewnić się, że wszystkie pliki tokenizerowe są dostępne. (2) Rozmiar i pamięć – konwersja dużych modeli bywa wymagająca pamięciowo. MLX-LM implementuje lazy loading wag, dzięki czemu nawet bardzo duże modele można konwertować na sprzęcie z ograniczoną RAM/VRAM. Wykorzystuje on możliwości safetensors do strumieniowego odczytu shardów (tzw. safe_open) i dzielenia obciążenia ￼. Dzięki temu, jak zauważyli użytkownicy, „w mlx-lm model ładuje się ‘leniwe’, podczas gdy w mlx-vlm trzeba załadować całość do pamięci” ￼. Oznacza to, że konwertując czysto tekstowe LLM, nie musimy mieć RAM równoważnego rozmiarowi modelu – MLX-LM poradzi sobie poprzez sukcesywne wczytywanie fragmentów. Mimo to, dobrze jest dysponować sporą pamięcią, zwłaszcza dla modeli 30B+. (3) Weryfikacja poprawności – po konwersji warto uruchomić model w MLX i sprawdzić przykładowe generacje czy odpowiedzi na znane pytania. W większości przypadków konwersja zachowuje dokładnie parametry modelu, więc jakość powinna być identyczna. Jeśli wystąpią rozbieżności, upewnijmy się, że model nie został przypadkowo zmodyfikowany (np. czy nie zastosowano kwantyzacji gdy nie chcieliśmy).

Konwersja modeli wizji komputerowej do formatu MLX (MLX-image i MLX-VLM)

Rodzaje modeli wizji: Modele CV mogą być jednomodalne (tylko obraz) – np. klasyfikatory pokroju ResNet, Vision Transformer, modele segmentacji (SAM) – lub multimodalne powiązane z językiem – np. CLIP i pochodne (SigLIP), obrazowo-tekstowe LLM-y (np. LLaVA, Gemma IT). Konwersja tych modeli do MLX odbywa się z użyciem odpowiednich narzędzi: dla czysto obrazowych modeli powstała biblioteka mlx-image (czasem zwana mlxim), a dla modeli łączących obraz z językiem – wspomniana biblioteka społecznościowa mlx-vlm.

Konwersja modeli obrazowych (mlxim): Biblioteka mlx-image autorstwa Riccardo Musmeci powstała na wzór PyTorchowej timm, dostarczając implementacje popularnych sieci CNN i wizji transformatorowej w MLX ￼. Oznacza to, że modele takie jak ResNet-18/50/152, ViT (różne rozmiary), Swin Transformer itp. zostały już zaimplementowane w MLX-image i udostępnione wraz z gotowymi wagami w Hubie (organizacja mlx-vision na Hugging Face) ￼ ￼. Dzięki temu konwersja tych standardowych modeli jest już wykonana – wystarczy pobrać model z Hub i użyć w MLX. Przykładowo, aby załadować pretrenowany ResNet-18 w MLX, wykonujemy: from mlxim.model import create_model; model = create_model("resnet18"), co automatycznie pobierze wagi z repozytorium mlx-vision/resnet18-mlxim ￼. Repozytorium mlx-vision zawiera obecnie dziesiątki skonwertowanych modeli (m.in. całą rodzinę ResNet i wiele wariantów ViT i Swin) ￼ ￼, osiągających na Apple Silicon podobne wyniki jak oryginały na GPU NVIDIA ￼. Jeśli chcemy przekonwertować nowy model wizji (np. EfficientNet czy ConvNeXt, jeśli nie ma w mlxim), istnieje możliwość wykorzystania analogicznej procedury co dla NLP: napisać kod ładujący model oryginalny (np. przez timm lub torchvision), utworzyć ekwiwalentny model MLX (bazując na mlxim) i zapisać wagi. Twórcy mlxim zachęcają społeczność do takich konwersji – istnieje osobna społeczność mlx-vision skupiająca kontrybutorów publikujących wagi modeli CV w formacie MLX ￼ ￼.

Przykład – Segment Anything (SAM): Model SAM (Segment Anything Model) od Meta AI to model segmentacji dowolnych obiektów z obrazów, składający się z potężnego enkodera wizji (ViT-huge) oraz prostego dekodera maski. W MLX zaimplementowano przykład wykorzystania SAM – jego architekturę odwzorowano w mlx-examples (folder segment_anything). Konwersję można przeprowadzić samemu lub pobrać gotowe wagi. Na GitHub pojawiło się repozytorium SAM-MLX z instrukcją „one-click” użycia SAM na Macu z MLX ￼ ￼. Wskazuje ono, jak zainstalować MLX, pobrać skrypt konwersji i albo ściągnąć już skonwertowany model sam-vit-base-mlx z Hugging Face, albo samodzielnie przekonwertować model SAM ViT z oryginalnych wag ￼. Takie inicjatywy pokazują, że nawet złożone modele CV można przenieść do MLX – wymaga to jednak trochę pracy (np. implementacji niestandardowych warstw). W przypadku SAM deweloperzy MLX dostarczyli gotowe komponenty, więc społeczność mogła szybko dokonać konwersji. Pułapki: Podczas konwersji modeli CV trzeba pamiętać o elementach takich jak: format wag (czy model oryginalny jest w PyTorch, JAX, czy TensorFlow – mlxim zakłada format PyTorchowy), ewentualne wagi pretreningowe (np. dla SAM trzeba było zaimportować wagi ViT-Huge). Ważna jest także kompatybilność tokenizera tekstowego dla modeli, które go używają (np. CLIP ma osobny tokenizer BPE – tutaj MLX-CLIP lub MLX-VLM muszą umieć go wczytać).

Konwersja modeli multimodalnych (wizja+tekst) – CLIP, SigLIP, Gemma 3 IT: Modele takie jak OpenAI CLIP czy Google SigLIP łączą dwie części – enkoder obrazu (zwykle ViT) i enkoder tekstu (np. Transformer językowy). Konwersja ich do MLX wymaga obsługi obu pod-modeli. Początkowo powstała biblioteka mlx-clip dedykowana uruchamianiu CLIP na Apple Silicon, jednak obecnie rozwój skupił się w projekcie MLX-VLM. MLX-VLM (stworzony przez społeczność, m.in. @Blaizzy) to pakiet umożliwiający inferencję i fine-tuning modeli VLM na Macu ￼. Obsługuje on wiele modeli wizyjno-językowych, jak np. PaliGemma (połączenie SigLIP i Gemma 2B), Qwen-VL, LLaVA, a ostatnio także Gemma 3.

Konwersja takiego modelu bywa nieco bardziej złożona niż czysto tekstowego, co widać na przykładzie SigLIP. SigLIP to ulepszona wersja CLIP z funkcją straty sigmoidalnej – architektonicznie niemal identyczna z CLIP ￼. Jeden z użytkowników zapytał, czy warto przekonwertować SigLIP do MLX – twórcy MLX odpowiedzieli entuzjastycznie, że jak najbardziej, i zasugerowali opublikowanie wyniku w organizacji mlx-community na HF ￼ ￼. Rzeczywiście, konwersji SigLIP dokonano w kontekście modelu PaliGemma – 3-miliardowego modelu Google łączącego SigLIP (Vision) z małym LLM Gemma (2B parametrów) ￼. Autor MLX-VLM dodał obsługę PaliGemma w wersji 0.1.0, co wymagało m.in. wsparcia dla architektury SigLIP i odpowiedniego łączenia dwóch części modelu ￼. W praktyce, konwertując CLIP/SigLIP, należy: (1) załadować część wizji (np. ViT-B/16) do MLX – można skorzystać z mlxim, bo ViT jest tam zaimplementowany, (2) załadować część tekstową (Transformer językowy, często Transformer B/32 lub podobny) – tu pomocne są komponenty MLX-LM, (3) połączyć je w jedną strukturę MLX-VLM, zapewniając wspólne działanie (np. normalizacja i iloczyn skalarny embedów). MLX-VLM dostarcza gotowe klasy do modeli multimodalnych znanych z literatury, co znacząco upraszcza zadanie. Narzędzia: MLX-VLM ma podobne narzędzie CLI do konwersji: mlx_vlm.convert. Użytkownicy zgłaszali jednak, że nie jest ono jeszcze tak dopracowane jak mlx_lm.convert – np. przy konwersji Qwen2-VL-72B pojawiały się błędy związane z pamięcią (ładowanie całości modelu naraz) ￼ ￼. Twórcy przyznają, że dla 72B modelu wizji+tekst aktualny proces wymaga maszyny zdolnej załadować ~120 GB model (72B FP16) – co mało kto posiada ￼. Jest to zatem obszar do optymalizacji (możliwe, że w przyszłości MLX-VLM również zaimplementuje lazy loading shardów). Niemniej jednak, mniejsze modele multimodalne (do ~30B) da się konwertować na mocniejszych Macach z 64GB RAM, zwłaszcza po kwantyzacji.

Przykład – Gemma 3 (Image+Text): Google wypuściło Gemma 3 w dwóch wersjach: PT (Pure Text) i IT (Image+Text). Wariant Gemma-3-27B-IT zawiera zdolność rozumienia obrazów (wykorzystuje enkoder wizji podobny do SigLIP) i generuje tekst. Taki model został już skonwertowany do formatu MLX tuż po premierze – na HF dostępne jest repozytorium mlx-community/gemma-3-27b-it-4bit z informacją: „model converted to MLX format from google/gemma-3-27b-it using mlx-vlm 0.1.17” ￼. Analogicznie tekstowy Gemma-3-27B-PT został udostępniony jako model MLX (quant 6-bit) ￼. Świadczy to o tym, że skrypt konwersji dla Gemma 3 zadziałał poprawnie. Kluczem było użycie mlx-vlm w wersji ≥0.1.17, która zawiera już wsparcie Gemma3. Z perspektywy poprawności implementacji convert_mlx-vlm.py dla Gemma-3-27B-PT – jeśli bazuje on na tej wersji biblioteki, jest on zgodny z aktualnymi technikami. Gemma PT jako model czysto językowy mógłby też zostać skonwertowany narzędziem MLX-LM (po dodaniu architektury do katalogu), ale zespół MLX zdecydował się użyć MLX-VLM nawet dla wariantu PT ￼. Prawdopodobnie wynikało to z faktu, że Gemma3 integruje pewne moduły multimodalne (np. embeeding obrazu, nawet jeśli nieużywane w PT), więc łatwiej było potraktować go spójnie narzędziem VLM. Rekomendacja: upewnij się, że używasz najnowszej wersji skryptu/biblioteki MLX-VLM; w razie problemów z brakiem pamięci spróbuj kwantyzować model w trakcie konwersji (np. do 4-bit), co dramatycznie zmniejszy jego rozmiar podczas zapisu.

Konwersja modeli głosowych (ASR/TTS) do formatu MLX-audio

Obsługiwane modele głosowe: MLX od niedawna rozszerza wsparcie na modele audio poprzez pakiet MLX-audio. Modele mowy obejmują zarówno rozpoznawanie mowy (ASR) – np. OpenAI Whisper, Meta SeamlessM4T (część ASR/translation) – jak i syntezę mowy (TTS) – np. Bark od Suno czy Kokoro TTS. Konwersja ich do MLX wymaga obsługi specyficznych architektur: enkoderów akustycznych (CNN/Transformer), dekoderów tekstowych lub wokoderów generujących falę audio.

Whisper (ASR): Whisper to model enkoder-dekoder przyjmujący spektrogram i zwracający tekst (transkrypcję). W 2022/23 MLX-community dokonała konwersji modeli Whisper – powstały repozytoria takie jak mlx-community/whisper-tiny-mlx, whisper-medium-mlx itp. W przypadku Whisper Medium (769M parametrów) wagi zapisano w pojedynczym pliku weights.npz ~1.52GB ￼ wraz z configiem. W tamtym czasie (rok temu) MLX nie miał jeszcze oddzielnej biblioteki audio – konwersji dokonano za pomocą przykładów w mlx-examples ￼. Użytkownik mógł skorzystać z interfejsu: import whisper; whisper.transcribe("file.wav") po zainstalowaniu MLX, co podmieniało backend na MLX GPU ￼. Obecnie pojawił się dedykowany pakiet mlx-audio, a w nim moduł ASR i TTS. Można się spodziewać, że Whisper zostanie włączony do MLX-audio (o ile już nie jest). Konwersja Whispera w praktyce polega na wyciągnięciu wszystkich wag modelu z PyTorch i zapisaniu w NPZ/safetensors – co dzięki prostej architekturze (Transformer encoder+decoder) jest względnie proste. Potencjalne pułapki: upewnienie się, że warstwa tokenizera mowy (Whisper używa tokenizer textowy, więc to akurat proste) jest poprawnie zaimportowana oraz że funkcje mel-spektrogramu są identycznie zaimplementowane (MLX może korzystać z gotowego kodu przetwarzania dźwięku lub wymagać dostarczenia spektrogramu jako wejście).

SeamlessM4T: to złożony, multimodalny system Meta AI obsługujący tłumaczenia między mową a tekstem (mowa→tekst, tekst→mowa, mowa→mowa itp.) ￼. W istocie SeamlessM4T składa się z kilku komponentów (ASR, moduł tłumaczący i TTS) w jednym modelu. Konwersja takiego giganta do MLX byłaby nietrywialna – musiałaby objąć wszystkie podmodele. Możliwe podejście to rozdzielenie zadań: np. wykorzystać MLX do części ASR (jak Whisper) i TTS, ale połączyć je w pipeline Pythonowy. Na chwilę obecną brak doniesień o pełnej konwersji SeamlessM4T do MLX – społeczność raczej skupia się na ułatwieniu uruchamiania go np. przez GGML/GGUF (na CPU) ￼. Jednak komponenty SeamlessM4T mogłyby być wsparte – np. jeżeli korzysta on z architektury podobnej do wstępnie trenowanych modeli mowy (HuBERT, w2v-BERT) to MLX mógłby je zaimplementować.

Bark (TTS): Bark to model generujący mowę na podstawie tekstu, działający w dwóch etapach: generacja tokenów semantycznych i akustycznych przez transformatory, a następnie dekodowanie ich do audio przez neurony dekodujące (wbudowany vocoder). Konwersja Bark do MLX wymagałaby przeniesienia obu transformatorów i ewentualnie warstwy dekodera. Póki co nie ma gotowego przykładu Bark-MLX, ale jest analogiczny projekt – Kokoro TTS. Kokoro to lekki model TTS (~82M) o wysokiej jakości mowy. Został on przekonwertowany i udostępniony jako mlx-community/Kokoro-82M-bf16, z informacją: „converted to MLX format from hexagrad/Kokoro-82M using mlx-audio 0.0.1” ￼. Użycie MLX-audio oznacza, że pakiet ten ma już funkcjonalność do generowania mowy. Rzeczywiście, sposobem użycia jest: pip install mlx-audio, a następnie python -m mlx_audio.tts.generate --model mlx-community/Kokoro-82M-bf16 --text "Hello world" ￼. To uruchamia model Kokoro w MLX i generuje dźwięk na Macu, co potwierdza sprawność konwersji. W przypadku Bark model jest większy (około 1GPT w sumie) i generuje audio 24 kHz – jego konwersja wymagałaby dopracowania wydajności. Niemniej MLX-audio pokazuje, że nawet syntezę mowy można efektywnie obsługiwać na GPU M1/M2.

Podsumowanie konwersji audio – best practices: Przy konwersji modeli mowy należy pamiętać o: (1) Pre- i postprocesingu – MLX może nie mieć gotowych funkcji do ekstrakcji cech audio (mel, MFCC itp.) lub odtworzenia audio z kodowania. Prawdopodobnie trzeba skorzystać z zewnętrznych narzędzi (librosa, torchaudio) lub implementacji w Pythonie. W przykładzie Kokoro, model oparty jest na architekturze TTS (prawdopodobnie podobnej do FastSpeech) – generuje on mel-spektrogram lub parametry, a następnie wymaga vocodera. Być może Kokoro zawiera uproszczony vocoder oparty na MLX. Użytkownik konwertujący Bark musiałby zaimplementować dekoder audio (np. HiFi-GAN) w MLX lub wykorzystać istniejący. (2) Tokenizery – modele mowy mogą używać niestandardowych tokenów (np. Bark ma tokeny semantyczne, nie czysto tekstowe). Należy dopilnować, by MLX potrafił je odczytać (np. Bark używa kodera AudioGPT – raczej to pozostawimy poza MLX, generując tokeny Bark offline). (3) Wydajność – generowanie audio to często setki lub tysiące kroków dekodera, co na GPU mobilnym może być wolne. Konieczna może być kwantyzacja lub redukcja modelu, by osiągnąć real-time. W Kokoro zastosowano bfloat16 (bf16), co sugeruje, że model był trenowany w bf16 – MLX pozwala zachować tę precyzję ￼. Dla większych modeli audio sensowne będzie użycie int8/int4.

Wspierane narzędzia i porównanie z innymi formatami

Ekosystem MLX vs PyTorch/TF/JAX: Konwersja do MLX odbywa się zwykle z modeli zaimplementowanych w PyTorch (czasem TensorFlow lub JAX). W praktyce Hugging Face Hub dostarcza interfejs ujednolicający – MLX korzysta z 🤗 Transformers do wczytania modelu niezależnie od frameworku źródłowego. Jeśli model bazowy jest w TF lub Flax, Transformers potrafi go przekonwertować do pytorchowego state_dict (o ile dostępne są wagi – HF często publikuje wagi we wszystkich formatach). Z punktu widzenia MLX nie ma to znaczenia – ważne by uzyskać tensory (numpy) do umieszczenia w MLX. W porównaniu, PyTorch nie wymaga takiej konwersji – uruchamiamy model bezpośrednio, ale ceną jest mniejsza optymalizacja na Mac (brak 4-bit inference out of the box, ograniczona wydajność MPS vs. Metal kernels MLX). TensorFlow i JAX także mogą uruchamiać modele, lecz ich formaty (SavedModel, Flax checkpoint) są inne i typowo nie kompatybilne z PyTorch bez konwersji – MLX omija te różnice poprzez wspomniane Transformers. Format MLX (safetensors) jest do pewnego stopnia analogiczny do pytorchowego state_dict – to po prostu kolekcja tablic z wagami. Różnica to dodanie meta-informacji jak typ modelu, co ułatwia ładowanie ￼. Trzeba zauważyć, że MLX nie konwertuje modelu do CoreML (formatu używanego na iPhonie). CoreML wymaga dodatkowej konwersji (np. przez coremltools), często z uproszczeniem modelu. MLX pozostaje w domenie NumPy/PyTorch-like, dając elastyczność podobną do uruchamiania modeli w Pythonie (można generować tekst, trenować dalsze etapy, itp.). Dlatego MLX to raczej alternatywa dla PyTorch na Mac, oferująca większą wydajność dzięki natywnej integracji z Metal i umożliwiająca łatwą kwantyzację modeli, co w standardowym PyTorch wymaga dodatkowych bibliotek (np. bitsandbytes).

Porównanie formatów wag: W MLX po konwersji otrzymujemy często kilka plików .safetensors plus plik index JSON. Jest to zbliżone do „shardów” modelu w PyTorch/HF. Np. Gemma-3-27B-6bit MLX składa się z 5 plików model-0000X-of-00005.safetensors (ok. 5.3 GB każdy) oraz indexu ￼ ￼. Taki podział pozwala obejść limit wielkości pojedynczego pliku i ułatwia pobieranie. W przeciwieństwie do PyTorch (gdzie typowo 1 plik .bin mieści całość, albo deweloper sam dzieli), MLX dzięki integracji z HF Hub automatycznie dzieli pliki powyżej pewnego rozmiaru. Safetensors jako format jest wspólny dla MLX i PyTorch (HF Transformers też go używa), więc teoretycznie można by odczytać wagi MLX w PyTorch – jednak uwaga: wagi mogą być zakodowane inaczej (np. spłaszczone, posegregowane). Dlatego do odwrotnej konwersji (MLX -> oryginał) służy opcja --dequantize i ewentualnie skrypty eksportu. Porównując z ONNX czy GGUF (formaty wymiany i inferencji), MLX nie dokonuje stałej kompilacji modelu – nadal możemy go modyfikować, fine-tunować. ONNX zamraża model i jest trudny do dalszego trenowania. GGUF (używany w llama.cpp) to silnie zoptymalizowany format binarny pod CPU/GPU, ale obsługuje głównie LLM-y i nie zachowuje wszystkiego (np. brak możliwości łatwej zmiany architektury). MLX stoi pośrodku – jest bardzo wydajny, ale wciąż utrzymuje model w formie gotowej do trenowania czy modyfikacji (np. w MLX można łatwo dokleić adapter LoRA i trenować – to dużą przewaga nad ONNX).

Optymalizacja modeli w MLX: kwantyzacja, grupowanie wag, precyzja obliczeń

Kwantyzacja podczas konwersji: Jedną z największych zalet MLX jest natywne wsparcie kwantyzacji modeli LLM. Już na etapie konwersji możemy wskazać flagę -q/--quantize, co spowoduje, że model zostanie zapisany w formie int4 (domyślnie) zamiast FP16 ￼ ￼. MLX implementuje algorytm 8/4-bit Symmetric Weight Quantization z grupowaniem wag (podobny do GPTQ). Domyślny rozmiar grupy to 64 elementy ￼, co oznacza, że np. dla każdej paczki 64 wag liniowej wyznaczany jest osobny skalar kwantyzacji – to zwiększa dokładność względem naiwnej kwantyzacji per-tensor. Użytkownik może regulować liczbę bitów parametrem --q-bits (2,3,4,6,8…) oraz wielkość grupy parametrem --q-group-size ￼ ￼. Domyślnie jest to 4 bity i grupa 64, co sprawdza się dla większości LLM.

Wiele modeli w Hubie MLX-community ma w nazwie -4bit czy -8bit, świadcząc o udanej kwantyzacji. Przykładowo, udostępniono model Qwen2-7B zarówno w wersji 8-bit jak i 4-bit ￼ ￼. Co więcej, społeczność eksperymentuje z nietypowymi bitwidths – np. Gemma-3-27B-PT-6bit, gdzie zastosowano 6-bitową kwantyzację wag (dając 6.64B efektywnych parametrów zamiast 27B) ￼. MLX umożliwia także kwantyzację mieszaną: zamiast jednorodnej liczby bitów dla wszystkich warstw, można podać tzw. predicate (--quant-predicate) określający, jakie warstwy kwantyzować mocniej, a jakie słabiej ￼. W zaawansowanych przykładach definiuje się np. schemat „2,6_mixed” – gdzie pewne wrażliwe warstwy zostają w 6-bitach, a reszta dostaje 2-bitową kwantyzację ￼. W modelu DeepSeek 32B skonwertowanym do MLX zaprezentowano całą gamę wariantów: oprócz domyślnego 4-bit, są pliki z wagami 3-bit, 6-bit, 8-bit oraz mieszane 2-6 bit, 3-6 bit, 4-6 bit itp. ￼ ￼. Takie podejście (multi-quantized model) pozwala użytkownikom dobrać najlepszy wariant – np. 2-bit dla maksymalnej oszczędności pamięci, 8-bit dla najwyższej dokładności ￼ ￼. Best practice przy kwantyzacji to testować model po – 4-bit zwykle zachowuje ~95-99% jakości oryginału przy ~2-4x mniejszym zużyciu VRAM, ale przy 3-bit czy 2-bit pewne pogorszenie jakości może być znaczące. Mieszane schematy próbują to kompensować (np. 2/6-bit miesza bardzo agresywną kwantyzację z łagodniejszą) ￼.

Grupowanie wag: Wspomniany parametr grupy (group_size) dotyczy tego, że MLX kwantyzuje wagi w blokach. Ustalono, że 64 to dobry kompromis – im mniejsza grupa (np. 32), tym więcej skal i offsetów trzeba zapisać (lekki narzut pamięci), ale tym precyzyjniej oddaje rozkład wag. Im większa grupa (128+), tym prostsza reprezentacja kosztem dokładności. MLX domyślnie używa 64, co pokrywa się z praktykami GPTQ. Użytkownik rzadko musi to zmieniać – chyba że chce eksperymentować. W MLX każda warstwa posiada metodę to_quantized(group_size, bits) która wykonywana jest automatycznie przy konwersji ￼. Dotyczy to głównie warstw liniowych i embedujących, bo one mają najwięcej wag ￼. Inne elementy (np. warstwy normujące, biasy) mogą pozostać w FP16, co nie wpływa znacząco na rozmiar.

Precyzja obliczeń: Modele w MLX mogą działać w różnych formatach: float32, float16, bfloat16, int8/4 (kwantyzacja). Apple GPU najlepiej radzi sobie z float16 – stąd default. Czasem jednak model może być oryginalnie w BF16 (np. niektóre modele Google, jak Gemma-2 27B BF16 ￼). MLX pozwala zachować bfloat16 podczas konwersji (--dtype bfloat16), dzięki czemu nie tracimy potencjalnej dokładności ￼. W praktyce M1/M2 nie ma natywnej arytmetyki BF16, więc te operacje wykonują się zapewne z konwersją, co może być odrobinę wolniejsze niż FP16. Dlatego większość modeli MLX-community jest w FP16 lub niżej. Jeśli chodzi o inny typ optymalizacji – np. pruning (przycinanie) – MLX nie oferuje automatycznego odchudzania modelu podczas konwersji. Można jednak przed konwersją zastosować narzędzia pruningu w PyTorch, a potem przekonwertować odchudzony model.

Ocena implementacji convert_mlx-vlm.py dla Gemma-3-27B-PT i rekomendacje

Na podstawie powyższych ustaleń można stwierdzić, że obecna implementacja konwersji Gemma 3 do MLX (zarówno wariantu PT, jak i IT) jest poprawna i zgodna z najnowszymi technikami, o ile korzysta z aktualnych bibliotek MLX. Udostępnione modele Gemma3-27B w Hubie (MLX 4-bit i 6-bit) działają, co dowodzi poprawności konwersji ￼. Implementacja w convert_mlx-vlm.py powinna zatem uwzględniać: wczytanie modelu przez Transformers (Google udostępniło Gemma3 na HF Hub z odpowiednią licencją), następnie stworzenie modelu MLX-VLM. Gemma-3-27B-PT jako czysto tekstowy można by też konwertować przez MLX-LM – jednak brak wsparcia w momencie premiery spowodował błąd „gemma3 not supported” ￼. Twórcy MLX poradzili użyć MLX-VLM dla wariantu multimodalnego, co zresztą zrobiono ￼. Z perspektywy użytkownika, obecne rozwiązanie (mlx-vlm 0.1.17+) spełnia swoją rolę. Optymalizacja skryptu: Jedyny zauważony mankament dotyczy dużych modeli multimodalnych – mlx_vlm.convert ładuje całość naraz, co przy 27B FP16 (~50 GB) jest na granicy możliwości typowego Mac Studio 64GB. W praktyce jednak Gemma3-27B przekonwertowano od razu do 4/6-bit, co zredukowało rozmiar ~4x, umożliwiając zmieszczenie modelu w RAM podczas zapisu. W przyszłości warto, aby implementacja convert_mlx-vlm.py zaadoptowała podejście lazy loading z MLX-LM (np. wczytywanie kolejnych shardów wag i zwalnianie nieużywanych). Mógłby to być potencjalny usprawniony pull request do repo MLX-VLM – co zasygnalizowano na forum (użytkownicy sugerowali zgłoszenie issue, by poprawić obsługę 72B modeli) ￼.

Rekomendacje odnośnie konwersji Gemma 3: Upewnij się, że w pliku convert_mlx-vlm.py poprawnie rozróżniasz wariant PT vs IT. Możliwe, że Gemma3 wymaga przekazania innego model type do MLX (np. gemma3_text vs gemma3 w MLX codebase). Według dyskusji deweloperów, dodano osobno gemma3_text (tekstowy) i gemma3 (multimodalny) w katalogu modeli ￼. Sprawdź więc, czy Twój skrypt wybiera właściwy identyfikator – inaczej może próbować wczytać nieistniejący komponent wizji dla PT. Jeśli chciałbyś używać Gemma3-27B-PT w aplikacjach czatowych (np. MLX-TextGen UI), prawdopodobnie wygodniej będzie po dodaniu wsparcia w MLX-LM używać go jako model tekstowy. Ale do generowania opisów obrazów – tylko MLX-VLM. Alternatywne podejścia: Gdyby okazało się, że MLX-VLM ma problemy, istnieje możliwość alternatywnej drogi: konwertować Gemma 3 do formatu Core ML i korzystać z biblioteki Core ML na Macu. Apple udostępniło nawet stable diffusion i Whisper w Core ML, lecz dla LLM (27B) Core ML nie jest praktyczne (ograniczenia pamięci Neural Engine, brak wygodnej obsługi promptów). Inna alternatywa to uruchomienie Gemma3 w środowisku Python/PyTorch z MPS – ale testy pokazują, że MLX jest szybszy i bardziej efektywny pamięciowo (dzięki wspomnianej kwantyzacji). Są też frameworki jak Ollama czy LM Studio, które wsparły format MLX, umożliwiając łatwe użycie lokalnych modeli. Widzieliśmy np., że użytkownicy LM Studio 0.3.13 zgłaszali poprawne działanie Gemma3 w runtime MLX 0.9.1, choć był bug generujący same <pad> – co jednak wynikało z ustawień promptu, a nie konwersji wag ￼ ￼. Konkluzja: obecna implementacja konwersji Gemma3 jest zasadniczo poprawna. Warto trzymać się oficjalnych narzędzi (mlx-lm, mlx-vlm, mlx-audio) i aktualizować je na bieżąco, bo społeczność i Apple aktywnie naprawiają błędy i dodają nowe funkcje.

Najlepsze otwarte repozytoria z przykładami: Na koniec, polecam zapoznać się z kilkoma źródłami, które mogą pomóc w dalszych konwersjach i optymalizacjach:
	•	Oficjalne repozytorium ml-explore/mlx-examples – zawiera przykłady konwersji i użycia wielu modeli (LLM, SAM, Whisper, LoRA itp.), często z komentarzami od autorów.
	•	Projekt Blaizzy/mlx-vlm – źródła biblioteki MLX-VLM, gdzie można śledzić zgłoszenia (issues) i rozwiązania dotyczące modeli multimodalnych. Wątki na GitHub poruszają np. temat SigLIP ￼ ￼ czy Qwen-VL.
	•	Zbiór Awesome-MLX – lista zasobów MLX, w tym narzędzia jak mlx-llm-server (serwer HTTP do serwowania modeli MLX), mlx-transformers (implementacje modeli w stylu Transformers na MLX), mlx-embedding-models (embeddingi BERT/RoBERTa na MLX) itp. ￼ ￼. To świetny punkt wyjścia do znalezienia gotowych rozwiązań.
	•	Hugging Face Hub organizacje: mlx-community (głównie LLM-y), mlx-vision (modele obrazowe), mlx-audio (modele mowy, np. kolekcja Kokoro TTS ￼). Przeglądając ich model cards można podpatrzeć parametry konwersji i osiągi. Na przykład w karcie DeepSeek 32B opisano dokładnie wszystkie warianty kwantyzacji i ich zastosowania ￼ ￼ – to kopalnia wiedzy o tym, jak kwantyzacja wpływa na model.
	•	Medium/TowardsDataScience – istnieje kilka artykułów omawiających MLX, np. „MLX: On-device machine learning on Apple Silicon” ￼ czy „Fine-tuning LLMs with Apple MLX locally”. Mogą one pomóc zrozumieć filozofię MLX oraz zawierają wskazówki praktyczne (np. konfiguracja środowiska, rozwiązywanie problemów z pamięcią).

Podsumowanie: Konwersja modeli NLP, wizji i mowy do formatu MLX-LM/VLM jest obecnie dobrze wspierana przez ekosystem narzędzi MLX. MLX-LM upraszcza przenoszenie LLM-ów i ich kwantyzację z minimalną utratą jakości, MLX-VLM oraz mlxim umożliwiają uruchamianie zaawansowanych modeli multimodalnych i wizualnych na Macach, a mlx-audio otwiera drzwi do lokalnego przetwarzania mowy. Obecna implementacja dla Gemma 3 korzysta z tych zdobyczy – jest poprawna i wykorzystuje najnowsze techniki (kwantyzacja 4–6 bit, safetensors, integracja z HF) zgodnie z najlepszymi praktykami. W miarę dalszego rozwoju MLX warto monitorować aktualizacje, zwłaszcza dotyczące optymalizacji konwertera VLM (pod kątem pamięciożerności) oraz wsparcia nowych modeli (np. przyszłe multimodalne LLM od Meta lub DeepMind). Ogólnie rekomendujemy kontynuować konwersje modeli z użyciem oficjalnych narzędzi MLX, dzielić się wynikami w społeczności (Hugging Face, GitHub) oraz testować różne ustawienia (bity, grupy) by znaleźć optymalny balans dla docelowych zastosowań ￼ ￼. Dzięki temu ekosystem MLX będzie się dalej dynamicznie rozwijał, oferując użytkownikom Mac coraz szersze możliwości uruchamiania nowoczesnych modeli AI lokalnie, bez konieczności korzystania z chmury.

Źródła: Przy opracowaniu powyższego zestawienia korzystano z oficjalnej dokumentacji Hugging Face dotyczącej MLX ￼ ￼, dyskusji i zgłoszeń w repozytoriach MLX na GitHub (ml-explore, Blaizzy) ￼ ￼, kart modeli MLX opublikowanych na Hugging Face Hub (Gemma 3, Whisper, Kokoro, DeepSeek) ￼ ￼ ￼, a także wypowiedzi użytkowników na forum Reddit dzielących się doświadczeniami z konwersją modeli (np. Qwen-VL 72B) ￼ ￼. Wszystkie te źródła potwierdzają rosnącą dojrzałość narzędzi MLX oraz skuteczność zastosowanych w nich technik konwersji i optymalizacji modeli.